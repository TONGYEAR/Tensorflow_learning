{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0,Testing Accuracy0.9541,Training Accuracy0.001\n",
      "Iter1,Testing Accuracy0.9661,Training Accuracy0.00095\n",
      "Iter2,Testing Accuracy0.9681,Training Accuracy0.0009025\n",
      "Iter3,Testing Accuracy0.9725,Training Accuracy0.000857375\n",
      "Iter4,Testing Accuracy0.9748,Training Accuracy0.00081450626\n",
      "Iter5,Testing Accuracy0.9756,Training Accuracy0.0007737809\n",
      "Iter6,Testing Accuracy0.9775,Training Accuracy0.0007350919\n",
      "Iter7,Testing Accuracy0.9743,Training Accuracy0.0006983373\n",
      "Iter8,Testing Accuracy0.9773,Training Accuracy0.0006634204\n",
      "Iter9,Testing Accuracy0.9768,Training Accuracy0.0006302494\n",
      "Iter10,Testing Accuracy0.9775,Training Accuracy0.0005987369\n",
      "Iter11,Testing Accuracy0.978,Training Accuracy0.0005688001\n",
      "Iter12,Testing Accuracy0.9784,Training Accuracy0.0005403601\n",
      "Iter13,Testing Accuracy0.9793,Training Accuracy0.0005133421\n",
      "Iter14,Testing Accuracy0.9791,Training Accuracy0.000487675\n",
      "Iter15,Testing Accuracy0.981,Training Accuracy0.00046329122\n",
      "Iter16,Testing Accuracy0.9789,Training Accuracy0.00044012666\n",
      "Iter17,Testing Accuracy0.9811,Training Accuracy0.00041812033\n",
      "Iter18,Testing Accuracy0.9815,Training Accuracy0.00039721432\n",
      "Iter19,Testing Accuracy0.9804,Training Accuracy0.0003773536\n",
      "Iter20,Testing Accuracy0.9817,Training Accuracy0.00035848594\n",
      "Iter21,Testing Accuracy0.9814,Training Accuracy0.00034056162\n",
      "Iter22,Testing Accuracy0.98,Training Accuracy0.00032353355\n",
      "Iter23,Testing Accuracy0.9811,Training Accuracy0.00030735688\n",
      "Iter24,Testing Accuracy0.9815,Training Accuracy0.000291989\n",
      "Iter25,Testing Accuracy0.9829,Training Accuracy0.00027738957\n",
      "Iter26,Testing Accuracy0.9804,Training Accuracy0.0002635201\n",
      "Iter27,Testing Accuracy0.982,Training Accuracy0.00025034408\n",
      "Iter28,Testing Accuracy0.9817,Training Accuracy0.00023782688\n",
      "Iter29,Testing Accuracy0.9825,Training Accuracy0.00022593554\n",
      "Iter30,Testing Accuracy0.9828,Training Accuracy0.00021463877\n",
      "Iter31,Testing Accuracy0.9822,Training Accuracy0.00020390682\n",
      "Iter32,Testing Accuracy0.982,Training Accuracy0.00019371149\n",
      "Iter33,Testing Accuracy0.982,Training Accuracy0.0001840259\n",
      "Iter34,Testing Accuracy0.9827,Training Accuracy0.00017482461\n",
      "Iter35,Testing Accuracy0.9823,Training Accuracy0.00016608338\n",
      "Iter36,Testing Accuracy0.9828,Training Accuracy0.00015777921\n",
      "Iter37,Testing Accuracy0.982,Training Accuracy0.00014989026\n",
      "Iter38,Testing Accuracy0.9822,Training Accuracy0.00014239574\n",
      "Iter39,Testing Accuracy0.9821,Training Accuracy0.00013527596\n",
      "Iter40,Testing Accuracy0.9813,Training Accuracy0.00012851215\n",
      "Iter41,Testing Accuracy0.9825,Training Accuracy0.00012208655\n",
      "Iter42,Testing Accuracy0.9826,Training Accuracy0.00011598222\n",
      "Iter43,Testing Accuracy0.982,Training Accuracy0.00011018311\n",
      "Iter44,Testing Accuracy0.9821,Training Accuracy0.000104673956\n",
      "Iter45,Testing Accuracy0.9818,Training Accuracy9.944026e-05\n",
      "Iter46,Testing Accuracy0.9823,Training Accuracy9.446825e-05\n",
      "Iter47,Testing Accuracy0.9818,Training Accuracy8.974483e-05\n",
      "Iter48,Testing Accuracy0.9826,Training Accuracy8.525759e-05\n",
      "Iter49,Testing Accuracy0.9832,Training Accuracy8.099471e-05\n",
      "Iter50,Testing Accuracy0.9828,Training Accuracy7.6944976e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "###载入数据集\n",
    "mnist= input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "###计算有多少批次\n",
    "batch_size=100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "###定义连个placeholder\n",
    "x= tf.placeholder(tf.float32,[None,784])\n",
    "y= tf.placeholder(tf.float32,[None,10])\n",
    "##再定义一个placehold\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "###优化的过程中定义一个学习率\n",
    "lr= tf.Variable(0.001,dtype= tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "###创建一个简单的神经网络\n",
    "\n",
    "W1= tf.Variable(tf.truncated_normal([784,500],stddev=0.1))\n",
    "b1=tf.Variable(tf.zeros([500])+0.1)\n",
    "L1=tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "###自定义的神经元drop out\n",
    "L1_drop=tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "W2= tf.Variable(tf.truncated_normal([500,300],stddev=0.1))\n",
    "b2=tf.Variable(tf.zeros([300])+0.1)\n",
    "L2=tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "###自定义的神经元drop out\n",
    "L2_drop=tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "\n",
    "W3= tf.Variable(tf.truncated_normal([300,10],stddev=0.1))\n",
    "b3=tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction= tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)\n",
    "\n",
    "\n",
    "###交叉熵代价函数\n",
    "# loss= tf.reduce_mean(tf.square(y- prediction))\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "###使用梯度下降法\n",
    "\n",
    "####\n",
    "train= tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###初始化变量\n",
    "init= tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "###结果存在在一个布尔型的变量中\n",
    "###tf.argmax是用来判断其中的最大值\n",
    "correct_prediciton = tf.equal(tf.arg_max(y,1),tf.arg_max(prediction,1))\n",
    "###求准确率\n",
    "###tf.cast是用来转化的\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediciton,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(51):\n",
    "        ###不断的减少学习率\n",
    "        sess.run(tf.assign(lr,0.001*(0.95 **epoch)))\n",
    "        for batch in range(n_batch):\n",
    "            ###首先获得一个批次，大小为100\n",
    "            batch_xs,batch_ys= mnist.train.next_batch(batch_size)\n",
    "            sess.run(train,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1})\n",
    "        \n",
    "        learning_rate= sess.run(lr)\n",
    "        acc=sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "       \n",
    "\n",
    "        print('Iter'+str(epoch)+',Testing Accuracy'+str(acc)+',Training Accuracy'+str(learning_rate))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
