{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0Testing Accuracy0.8474\n",
      "Iter1Testing Accuracy0.8959\n",
      "Iter2Testing Accuracy0.9023\n",
      "Iter3Testing Accuracy0.9051\n",
      "Iter4Testing Accuracy0.9084\n",
      "Iter5Testing Accuracy0.9097\n",
      "Iter6Testing Accuracy0.9124\n",
      "Iter7Testing Accuracy0.9143\n",
      "Iter8Testing Accuracy0.9147\n",
      "Iter9Testing Accuracy0.9168\n",
      "Iter10Testing Accuracy0.9184\n",
      "Iter11Testing Accuracy0.9185\n",
      "Iter12Testing Accuracy0.9184\n",
      "Iter13Testing Accuracy0.9183\n",
      "Iter14Testing Accuracy0.9204\n",
      "Iter15Testing Accuracy0.9204\n",
      "Iter16Testing Accuracy0.9206\n",
      "Iter17Testing Accuracy0.9211\n",
      "Iter18Testing Accuracy0.9212\n",
      "Iter19Testing Accuracy0.9215\n",
      "Iter20Testing Accuracy0.9214\n",
      "Iter21Testing Accuracy0.9214\n",
      "Iter22Testing Accuracy0.9217\n",
      "Iter23Testing Accuracy0.9233\n",
      "Iter24Testing Accuracy0.9226\n",
      "Iter25Testing Accuracy0.9231\n",
      "Iter26Testing Accuracy0.9236\n",
      "Iter27Testing Accuracy0.923\n",
      "Iter28Testing Accuracy0.9243\n",
      "Iter29Testing Accuracy0.9234\n",
      "Iter30Testing Accuracy0.9237\n",
      "Iter31Testing Accuracy0.9243\n",
      "Iter32Testing Accuracy0.9244\n",
      "Iter33Testing Accuracy0.9239\n",
      "Iter34Testing Accuracy0.9251\n",
      "Iter35Testing Accuracy0.9258\n",
      "Iter36Testing Accuracy0.9256\n",
      "Iter37Testing Accuracy0.925\n",
      "Iter38Testing Accuracy0.9249\n",
      "Iter39Testing Accuracy0.9258\n",
      "Iter40Testing Accuracy0.9257\n",
      "Iter41Testing Accuracy0.9256\n",
      "Iter42Testing Accuracy0.9253\n",
      "Iter43Testing Accuracy0.9267\n",
      "Iter44Testing Accuracy0.9262\n",
      "Iter45Testing Accuracy0.9274\n",
      "Iter46Testing Accuracy0.9259\n",
      "Iter47Testing Accuracy0.9267\n",
      "Iter48Testing Accuracy0.9264\n",
      "Iter49Testing Accuracy0.9262\n",
      "Iter50Testing Accuracy0.9272\n"
     ]
    }
   ],
   "source": [
    "###第二波改进增加多个命名空间\n",
    "\n",
    "\n",
    "\n",
    "###载入数据集\n",
    "mnist= input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "\n",
    "\n",
    "###计算有多少批次\n",
    "batch_size=100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "\n",
    "\n",
    "##来个函数\n",
    "def variable_summariess(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean=tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev=tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram',var)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###命名空间\n",
    "with tf.name_scope('input'):\n",
    "    ###定义连个placeholder\n",
    "    x= tf.placeholder(tf.float32,[None,784],name='x_input')\n",
    "    y= tf.placeholder(tf.float32,[None,10],name='y_input')\n",
    "###创建一个简单的神经网络\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('Weight'):\n",
    "        W= tf.Variable(tf.zeros([784,10]),name='Weight_1')\n",
    "        variable_summariess(W)\n",
    "    with tf.name_scope('biases'):\n",
    "        b=tf.Variable(tf.zeros([10]))\n",
    "        variable_summariess(b)\n",
    "    with tf.name_scope('mut'):\n",
    "        mut=tf.matmul(x,W)+b\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction= tf.nn.softmax(mut)\n",
    "\n",
    "\n",
    "###二次代价函数\n",
    "with tf.name_scope('loss'):\n",
    "    \n",
    "# loss= tf.reduce_mean(tf.square(y- prediction))\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "###使用梯度下降法\n",
    "with tf.name_scope('train'):\n",
    "    train= tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "###初始化变量\n",
    "init= tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    ###结果存在在一个布尔型的变量中\n",
    "    ###tf.argmax是用来判断其中的最大值\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediciton = tf.equal(tf.arg_max(y,1),tf.arg_max(prediction,1))\n",
    "    ###求准确率\n",
    "    ###tf.cast是用来转化的\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediciton,tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "###产生metadata文件\n",
    "if tf.gfile.Exists(DeprecationWarning)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "####合并所有的summary\n",
    "merged=tf.summary.merge_all()\n",
    "        \n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer= tf.summary.FileWriter('logs/',sess.graph)\n",
    "    for epoch in range(51):\n",
    "        for batch in range(n_batch):\n",
    "            ###首先获得一个批次，大小为100\n",
    "            batch_xs,batch_ys= mnist.train.next_batch(batch_size)\n",
    "            summary,_=sess.run([merged,train],feed_dict={x:batch_xs,y:batch_ys})\n",
    "        writer.add_summary(summary,epoch)    \n",
    "        acc=sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print('Iter'+str(epoch)+'Testing Accuracy'+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "###为了看到更多的点可以使用\n",
    "for i in range(2000)：\n",
    "    batch_xs,batch_ys=mnist.train.next_batch(100)\n",
    "    summary,_=sess.run([merged,train],feed_dict={x:batch_xs,y:batch_ys})\n",
    "    writer.add_summary(summary,i)\n",
    "    if i%500 == 0：\n",
    "        print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
